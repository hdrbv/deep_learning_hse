{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: \n",
    "\n",
    "Для каждой домашней работы обозначаются мягкие и жесткие дедлайны. За каждый день просрочки после мягкого дедлайна снимается 20% от оценки.\n",
    "\n",
    "После жёсткого дедлайна работы не принимаются. Даже при опоздании на одну секунду. Сдавайте заранее.\n",
    "\n",
    "Cтудент может 1 раз сдать домашнее задание после мягкого дедлайна (но до жёсткого) без штрафов.\n",
    "\n",
    "В случае использования больших языковых моделей (large language models (LLMs), к примеру: ChatGPT, GigaChat, Qwen, etc):\n",
    "- В chunk (ячейку) выше кода, созданного LLM, прикрепляйте промпт, который использовался для генерации.\n",
    "- Отдельно опишите как подбирали промпты, какие заметили преимущества и недостатки GenAI для данного задания.\n",
    "- За решение с указанным промптом - _штраф 40%_ для конкретного задания, который может быть пересмотрен в сторону увеличения в следующих случаях: \n",
    "    - использован ответ LLM без указания промпта _(штраф 100%)_\n",
    "    - решение избыточно и, или написано неоптимально (использование magic команд без необходимости, использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, etc) _(штраф 50%)_\n",
    "\n",
    "Также: \n",
    "- Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
    "- Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
    "- Мы в любом случае оставляем за собой право пригласить студента для защиты своего ДЗ, если заподозрим плагиат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQl6BxbTv-eK"
   },
   "source": [
    "# Activations\n",
    "\n",
    "В этом разделе напишем собственную реализацию функций активации\n",
    "\n",
    "\n",
    "Запрещено внутри своей реализации создавать класс активации из pytorch и просто применять его. Разрешено исползовать простые ф-ии pytorch типа [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html) и т д\n",
    "\n",
    "Если у ф-ии активации есть дополнительные аргументы, значение по умолчанию должно быть такое же как в реализации PyTorch\n",
    "\n",
    "**Материалы по pytorch:**\n",
    "\n",
    "* [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNMlqoCiv-eM"
   },
   "source": [
    "## Prerequirements\n",
    "\n",
    "```\n",
    "pip install torch numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR_-L-78De7T"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/60658965/7286121\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T12:02:03.697132Z",
     "iopub.status.busy": "2021-01-24T12:02:03.696647Z",
     "iopub.status.idle": "2021-01-24T12:02:05.150001Z",
     "shell.execute_reply": "2021-01-24T12:02:05.148949Z",
     "shell.execute_reply.started": "2021-01-24T12:02:03.697089Z"
    },
    "id": "ovtEtMOxv-eN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGTRWWFo5pOy"
   },
   "source": [
    "## Задание 1\n",
    "Напишите свою версию версию функции активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rZXbfck6WPO"
   },
   "outputs": [],
   "source": [
    "%%write_and_run myrelu.py \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyReLU(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # hint! Входной тензор нужно скопировать,\n",
    "        # чтобы действиями внутри этого метода не привели к изменению внешнего аргумента\n",
    "        input_clone = torch.clone(input)\n",
    "        # ...\n",
    "        # return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRV-Ea2N5zDb"
   },
   "source": [
    "## Задание 2\n",
    "Напишите свою версию версию функции активации [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ouIrrlq63uN"
   },
   "outputs": [],
   "source": [
    "%%write_and_run myleakyrelu.py \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLeakyReLU(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # hint! Входной тензор нужно скопировать,\n",
    "        # чтобы действиями внутри этого метода не привели к изменению внешнего аргумента\n",
    "        input_clone = torch.clone(input)\n",
    "        # ...\n",
    "        # return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS2RXTz_55iS"
   },
   "source": [
    "## Задание 3\n",
    "Напишите свою версию версию функции активации [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTHnuZQA67ga"
   },
   "outputs": [],
   "source": [
    "%%write_and_run mysigmoid.py \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MySigmoid(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # для этого класса копировать входной тензор уже нет необходимости, почему?\n",
    "        # ...\n",
    "        # return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGZs035Rv-eS"
   },
   "source": [
    "## Задание 4\n",
    "Напишите свою версию версию функции активации [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT1IEmzFv-eT"
   },
   "outputs": [],
   "source": [
    "%%write_and_run myelu.py \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyELU(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # для этого класса копировать входной тензор уже нет необходимости, почему?\n",
    "        # ...\n",
    "        # return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUfu8arDKoNm"
   },
   "source": [
    "## Тест\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAagGpwSKqBN"
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def _check_pytorch_module_was_not_used(file, module):\n",
    "\n",
    "    file = open(file, mode='r')\n",
    "    \n",
    "    assert module not in file.read(), \"pytorch module must not be used in you activation implementation\"\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "    return\n",
    "\n",
    "def _test_activation(myactivation, torch_activation):\n",
    "    print(myactivation)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        randinput = torch.rand([100])\n",
    "        myactivation_output = myactivation(randinput)\n",
    "\n",
    "        assert id(myactivation_output) != id(randinput), 'pytorch activation function must return new tensor'\n",
    "\n",
    "        for _ in range(100):\n",
    "            randinput = torch.rand([5, 5, 5])\n",
    "\n",
    "            assert torch.allclose(myactivation(randinput), torch_activation(randinput)), 'activation output is not equals to touch ones output'\n",
    "\n",
    "def test_relu():\n",
    "    from myrelu import MyReLU\n",
    "\n",
    "    my_activation = MyReLU()\n",
    "    \n",
    "    _check_pytorch_module_was_not_used(\"myrelu.py\", '.ReLU(')\n",
    "    _test_activation(my_activation, nn.ReLU())\n",
    "\n",
    "def test_leaky_relu():\n",
    "    from myleakyrelu import MyLeakyReLU\n",
    "\n",
    "    my_activation = MyLeakyReLU()\n",
    "    \n",
    "    _check_pytorch_module_was_not_used(\"myleakyrelu.py\", '.LeakyReLU(')\n",
    "    _test_activation(my_activation, nn.LeakyReLU())\n",
    "\n",
    "def test_sigmoid():\n",
    "    from mysigmoid import MySigmoid\n",
    "\n",
    "    my_activation = MySigmoid()\n",
    "    \n",
    "    _check_pytorch_module_was_not_used(\"mysigmoid.py\", '.MySigmoid(')\n",
    "    _test_activation(my_activation, nn.Sigmoid())\n",
    "\n",
    "def test_elu():\n",
    "    from myelu import MyELU\n",
    "\n",
    "    my_activation = MyELU()\n",
    "    \n",
    "    _check_pytorch_module_was_not_used(\"myelu.py\", '.MyELU(')\n",
    "    _test_activation(my_activation, nn.ELU())\n",
    "\n",
    "\n",
    "test_relu()\n",
    "test_leaky_relu()\n",
    "test_sigmoid()\n",
    "test_elu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpO0-LSGIac0"
   },
   "source": [
    "# Weight Initialization\n",
    "В этом ноутбуке вы узнаете, как найти хорошие начальные веса для нейронной сети. \n",
    "Инициализация весов происходит один раз, когда модель создана, до обучения.\n",
    "Имея хорошие начальные веса, можно расположить нейронную сеть близко к оптимальному решению.\n",
    "Это позволяет нейронной сети быстрее сойтись к наилучшему решению.\n",
    "\n",
    "\n",
    "## Initial Weights and Observing Training Loss\n",
    "\n",
    "Чтобы увидеть, как работают различные веса, мы протестируем один и тот же набор данных и нейронную сеть. Таким образом, мы знаем, что любые изменения в поведении модели происходят из-за весов, а не из-за каких-либо изменений данных или структуры модели. \n",
    "\n",
    "### Dataset and Model\n",
    "\n",
    "Для изучения различных инициализаций мы обучим MLP классифицировать изображения из набора данных [Fashion-MNIST] (https://github.com/zalandoresearch/fashion-mnist). Набор данных FashionMNIST содержит изображения типов одежды; ' classes = ['футболка / топ', 'брюки', 'пуловер', 'платье', 'пальто', 'сандалии', 'рубашка', 'кроссовки', 'сумка',`ботильоны']'. Изображения нормализуются таким образом, чтобы их пиксельные значения находились в диапазоне [0.0 - 1.0).  Запустите ячейку ниже, чтобы загрузить данные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pl7r9RDIac1"
   },
   "source": [
    "### Import Libraries and Load [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xNOEfk7Iac2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.FashionMNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtSyj5oPIadP"
   },
   "source": [
    "### Visualize Some Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4_Xhj00IadP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, int(20/2), idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A_HQmOHIadV"
   },
   "source": [
    "## Архитектура модели\n",
    "\n",
    "Мы создадим MLP (multilayer perceptron), который будем использовать для классификации данных, со следующими характеристиками:\n",
    "\n",
    "\n",
    "* 3 линейных слоя с размерами 256 и 128; \n",
    "\n",
    "* MLP принимает в качестве входных данных выпрямленное изображение (вектор длины 784) и выдает оценку принадлежности объекта к каждому из 10 классов.\n",
    "---\n",
    "Мы проверим влияние различных инициализаций на эту 3-слойную нейронную сеть, обученную с активациями ReLU и оптимизатором Adam.  \n",
    "\n",
    "Полученные выводы применимы и к другим нейронным сетям, включая различные активации и оптимизаторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVgjRR_QIadW"
   },
   "source": [
    "---\n",
    "\n",
    "### All Zeros or Ones\n",
    "Следуя принципам бритвы Оккама ([Occam's razor](https://en.wikipedia.org/wiki/Occam's_razor)), вы могли бы естестевенно подумать, что достаточно проиницализировать все веса нулем или единицей.\n",
    "\n",
    "При одинаковом весе все нейроны в каждом слое выдают одинаковый результат.  Это затрудняет обучение, так как непонятно, какие именно веса в какую сторону нужно менять.\n",
    "\n",
    "Давайте сравним функции потерь для двух моделей, проинициализированных (1) нулями и (2) единицами.\n",
    "\n",
    "Ниже мы используем Pytorch's [nn. init](https://pytorch.org/docs/stable/nn.html#torch-nn-init), чтобы проинициализировать веса каждого линейного слоя константной. Библиотека init предоставляет ряд функций инициализации, которые дают возможность инициализировать веса каждого слоя в соответствии с его типом.\n",
    "Для линейного слоя веса инициализируются следующим образом:\n",
    "\n",
    ">```\n",
    "if isinstance(m, nn.Linear):\n",
    "    nn.init.constant_(m.weight, constant_weight)\n",
    "    nn.init.constant_(m.bias, 0)\n",
    "```\n",
    "\n",
    "где `constant_weight` - значение константы (в нашем случае 0 или 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWNwHkLuti_Q"
   },
   "source": [
    "**Задание**: определите модель c описанной выше архитуктурой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JLlJa8FIadZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):\n",
    "        super(Net, self).__init__()\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = \n",
    "        # linear layer (hidden_1 -> hidden_2)\n",
    "        self.fc2 = \n",
    "        # linear layer (hidden_2 -> 10)\n",
    "        self.fc3 =\n",
    "        # dropout layer (p=0.2)\n",
    "        self.dropout =\n",
    "        \n",
    "        # initialize the weights to a specified, constant value\n",
    "        if(constant_weight is not None):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.constant_(m.weight, constant_weight)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "            \n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IaEhz74Iadg"
   },
   "source": [
    "### Сравнение поведения модели\n",
    "\n",
    "Ниже мы используем функцию `.compare_init_weights`, чтобы сравнить функции потерь на обучении и тесте для двух моделей: `model_0` и `model_1`.  Эта функция принимает список моделей (каждая с различными начальными весами), название создаваемого графика, а также загрузчики обучающих и тестовых наборов данных. Для каждой заданной модели эта функцию построит график лосса га обучения для первых 100 батчей и выведет точность валидации после 2 эпох обучения. \n",
    "\n",
    "*Примечание: Если вы использовали батчи меньшего размера, вы можете увеличить количество эпох здесь, чтобы лучше сравнить, как ведут себя модели после просмотра нескольких сотен изображений.* \n",
    "\n",
    "\n",
    "**Задание**: Допишите обучение модели и запустите ячейки ниже, чтобы увидеть разницу между инициализациями всеми нулями и всеми единицами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDxbliCuIadh"
   },
   "outputs": [],
   "source": [
    "# initialize two NN's with 0 and 1 constant weights\n",
    "model_0 = Net(constant_weight=0)\n",
    "model_1 = Net(constant_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrmqVW1MKWHT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _get_loss_acc(model, train_loader, valid_loader):\n",
    "    \"\"\"\n",
    "    Get losses and validation accuracy of example neural network\n",
    "    \"\"\"\n",
    "    n_epochs = 2\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Training loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    # Measurements used for graphing loss\n",
    "    loss_batch = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize var to monitor training loss\n",
    "        train_loss = 0.0\n",
    "        ########################\n",
    "        # TODO train the model #\n",
    "        ########################\n",
    "        for data, target in train_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = \n",
    "            # calculate the batch loss\n",
    "            loss = \n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            # perform a single optimization step (parameter update)\n",
    "            # record average batch loss \n",
    "             \n",
    "    # after training for 2 epochs, check validation accuracy \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # get the predicted class from the maximum class score\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # count up total number of correct labels\n",
    "        # for which the predicted and true labels are equal\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum()\n",
    "      \n",
    "    # calculate the accuracy\n",
    "    # to convert `correct` from a Tensor into a scalar, use .item()\n",
    "    valid_acc = correct.item() / total\n",
    "\n",
    "    # return model stats\n",
    "    return loss_batch, valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKNMCYznIadn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compare_init_weights(\n",
    "        model_list,\n",
    "        plot_title,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        plot_n_batches=100):\n",
    "    \"\"\"\n",
    "    Plot loss and print stats of weights using an example neural network\n",
    "    \"\"\"\n",
    "    colors = ['r', 'b', 'g', 'c', 'y', 'k']\n",
    "    label_accs = []\n",
    "    label_loss = []\n",
    "\n",
    "    assert len(model_list) <= len(colors), 'Too many initial weights to plot'\n",
    "\n",
    "    for i, (model, label) in enumerate(model_list):\n",
    "        torch.save(model.state_dict(), f\"{label}.pt\")\n",
    "\n",
    "        loss, val_acc = _get_loss_acc(model, train_loader, valid_loader)\n",
    "\n",
    "        plt.plot(loss[:plot_n_batches], colors[i], label=label)\n",
    "        label_accs.append((label, val_acc))\n",
    "        label_loss.append((label, loss[-1]))\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "    print('After 2 Epochs:')\n",
    "    print('Validation Accuracy')\n",
    "    for label, val_acc in label_accs:\n",
    "        print('  {:7.3f}% -- {}'.format(val_acc*100, label))\n",
    "    print('Training Loss')\n",
    "    for label, loss in label_loss:\n",
    "        print('  {:7.3f}  -- {}'.format(loss, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4H_ov6gKPC2"
   },
   "outputs": [],
   "source": [
    "model_list = [(model_0, 'All Zeros'),   # не надо менять название лейбла модели, на это завязаны тесты\n",
    "              (model_1, 'All Ones')]    # не надо менять название лейбла модели, на это завязаны тесты\n",
    "\n",
    "compare_init_weights(model_list, \n",
    "                             'All Zeros vs All Ones', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMZF8HhhIadp"
   },
   "source": [
    "Как вы можете видеть, точность близка к случайному угадыванию как для нулей, так и для единиц, около 10%.\n",
    "\n",
    "Нейронной сети трудно определить, какие веса должны быть изменены, так как нейроны имеют одинаковый выход для каждого слоя.  Чтобы избежать нейронов с одинаковым выходом, давайте использовать уникальные веса.  Мы также можем случайным образом выбрать веса, чтобы избежать застревания в локальном минимуме для каждого запуска.\n",
    "\n",
    "Хорошим решением для получения этих случайных весов является выборка из однородного распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sQXCy-4LUq-"
   },
   "source": [
    "# Воспроизводимость результатов\n",
    "\n",
    "Для того, чтобы запуск в колабе и запуск в пайплайных были более-менее воспроизводимы, воспользуйтесь следующей функцией. См так же [заметку](https://pytorch.org/docs/stable/notes/randomness.html) о воспроизводимости в док. pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vun0jePgLSou"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    if torch.__version__ >= '1.8':\n",
    "        torch.use_deterministic_algorithms()\n",
    "    else:\n",
    "        torch.set_deterministic(True)\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "aa4-lO_dIadq"
   },
   "source": [
    "### Равномерное распределение\n",
    "[Равномерное распределение](https://en.wikipedia.org/wiki/Uniform_distribution) имеет равную вероятность выбора любого числа из набора. Мы будем выбирать из непрерывного распределения, поэтому вероятность выбора одного и того же числа невелика. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B_gpGUMIadu"
   },
   "source": [
    "### Uniform Initialization, Baseline\n",
    "\n",
    "\n",
    "Давайте посмотрим, насколько хорошо нейронная сеть тренируется с использованием равномерной инициализации весов, где параметры равномерного распределения `a=0.0` и `b=1.0`. Мы рассмотрим другой способ инициализации весов нейросети (помимо использованного в коде класса Net). Чтобы инициализировать веса вне определения модели, вы можете:\n",
    "1. Определить функцию, которая инициализирует веса нужных слоев (в нашем случае - линейных)\n",
    "\n",
    "2. Инициализировать модель, используя `model.apply(fn)`, которая применяет функцию `fn` к каждому слою модели.\n",
    "\n",
    "Для равномерной инициализации весов нашей модели используйте `weight.data.uniform_`.\n",
    "\n",
    "**Задание:** допишите функцию равномерной инициализации весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vgzzwcqIadv"
   },
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DvzLfUwIady"
   },
   "outputs": [],
   "source": [
    "# create a new model with these weights\n",
    "model_uniform = Net()\n",
    "\n",
    "seed_everything(42)\n",
    "model_uniform.apply(weights_init_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJ9tT12SIaeG"
   },
   "outputs": [],
   "source": [
    "# evaluate behavior \n",
    "compare_init_weights([(model_uniform, 'Uniform Weights')],  # не надо менять название лейбла модели, на это завязаны тесты\n",
    "                             'Uniform Baseline', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdMazkSiIaeX"
   },
   "source": [
    "---\n",
    "График потерь показывает, что нейронная сеть учится, чего она не делала со всеми нулями или со всеми единицами. Мы движемся в правильном направлении!\n",
    "\n",
    "## Общее правило инициализации весов\n",
    "Общее правило для инициализации весов в нейронной сети состоит в том, чтобы установить их близкими к нулю, но не слишком маленькими. \n",
    ">Хорошая практика заключается в том, чтобы инициализировать веса в диапазоне $[- y, y]$, где $y=1/\\sqrt{n}$  \n",
    "($n$ - это число входов в данный нейрон).\n",
    "\n",
    "Давайте посмотрим, верно ли это: центрируем наш равномерный диапазон относительно нуля, сдвинув его на 0,5.  Это даст нам диапазон [-0.5, 0.5] равномерного распределения.\n",
    "\n",
    "**Задание:** поменяйте функцию равномерной инициализации весов, чтобы распределение весов было в диапозоне [-0.5, 0.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxQzgLVrIaeZ"
   },
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_center(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    " \n",
    "# create a new model with these weights\n",
    "model_centered = Net()\n",
    "\n",
    "seed_everything(42)\n",
    "model_centered.apply(weights_init_uniform_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n88LC249Iaei"
   },
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "\n",
    "# create a new model with these weights\n",
    "model_rule = Net()\n",
    "model_rule.apply(weights_init_uniform_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dxc-99kCIaem"
   },
   "outputs": [],
   "source": [
    "model_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'),  # не надо менять название лейбла модели, на это завязаны тесты\n",
    "              (model_rule, 'General Rule [-y, y)')]              # не надо менять название лейбла модели, на это завязаны тесты\n",
    "\n",
    "compare_init_weights(model_list, \n",
    "                             '[-0.5, 0.5) vs [-y, y)', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZZ_3YtuIaep"
   },
   "source": [
    "Такое поведение действительно многообещающе! Мало того, что лосс уменьшается, но, кажется, это происходит очень быстро; всего через две эпохи мы получаем довольно высокую точность на тесте. Это должно дать вам некоторое представление о том, почему хорошая инициализация весов действительно может помочь тренировочному процессу!\n",
    "\n",
    "---\n",
    "\n",
    "Равномерное распределение имеет одинаковый шанс выбрать *любое значение* в диапазоне. Что, если мы используем распределение, которое имеет более высокий шанс выбрать числа ближе к 0?  Давайте рассмотрим на нормальное распределение.\n",
    "\n",
    "### Hормальное распределение\n",
    "В отличие от равномерного распределения, [нормальное распределение](https://en.wikipedia.org/wiki/Normal_distribution) имеет более высокую вероятность выбора числа, близкого к среднему значению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6KlG2T2Iaer"
   },
   "source": [
    "**Задание:** добавьте нормальное распределение:\n",
    "как стандартное отклонение выберите $y=1/\\sqrt{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWtr-z3WIaes"
   },
   "outputs": [],
   "source": [
    "## complete this function\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iriwx51SIaev"
   },
   "outputs": [],
   "source": [
    "## -- no need to change code below this line -- ##\n",
    "\n",
    "# create a new model with the rule-based, uniform weights\n",
    "model_uniform_rule = Net()\n",
    "\n",
    "seed_everything(42)\n",
    "model_uniform_rule.apply(weights_init_uniform_rule)\n",
    "\n",
    "# create a new model with the rule-based, NORMAL weights\n",
    "model_normal_rule = Net()\n",
    "\n",
    "seed_everything(42)\n",
    "model_normal_rule.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u52PY05QIaez"
   },
   "outputs": [],
   "source": [
    "model_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), # не надо менять название лейбла модели, на это завязаны тесты\n",
    "              (model_normal_rule, 'Normal Distribution')]   # не надо менять название лейбла модели, на это завязаны тесты\n",
    "\n",
    "compare_init_weights(model_list, \n",
    "                             'Uniform vs Normal', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhmqCw3ZIae2"
   },
   "source": [
    "Нормальное распределение дает нам довольно похожее поведение по сравнению с равномерным распределением в данном случае. Вероятно, это связано с тем, что наша сеть очень мала; более крупная нейронная сеть будет выбирать больше весовых значений из каждого из этих распределений, увеличивая эффект обоих стилей инициализации. В общем случае нормальное распределение приведет к улучшению качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.01 балл)** Вставьте изображение, описывающее вас в процессе выполнения данного домашнего задания 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.tenor.com/ZCpXCL6vnDoAAAAC/minion-funny.gif\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
